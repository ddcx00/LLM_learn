# 第五章 搭建大模型

## 5.1 实现LLaMA2大模型
 LLaMA2模型结构如下
<div>
<image src="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/5-images/LLama2.png">
</div>

### 5.1.1 定义超参数

定义的超参数，包括有模型的大小，层数，头数，词嵌入维度，隐藏层维度等。 这些超参数可以根据实际情况进行调整。

这里我们自定义一个ModelConfig 类，来存储和记录我们的超参数，这里我们继承PretrainedConfig类，这是transformers库中的参数类，通过继承这个类，来方便使用transformers库中的一些功能，也方便后续导出Hugging Face模型

```python
from transformers import PretrainedConfig

class ModelConfig(PretrainedConfig):
    
    def __init__(
        self,
        dim: int = 768, # 模型维度
        n_layers: int = 12, # Transformer的层数
        n_heads: int = 16, # 注意力机制的头数
        n_kv_heads: int = 8, # 键值头的数量
        vocab_size: int = 6144, # 词汇表大小
        hidden_dim: int = None, # 隐藏层维度
        multiple_of: int = 64,
        norm_eps: float = 1e-5, #
    ):

```