# 第五章 搭建大模型

## 5.1 实现LLaMA2大模型
 LLaMA2模型结构如下
<div>
<image src="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/5-images/LLama2.png">
</div>

### 5.1.1 定义超参数

定义的超参数，包括有模型的大小，层数，头数，词嵌入维度，隐藏层维度等。 这些超参数可以根据实际情况进行调整。

这里我们自定义一个ModelConfig 类，来存储和记录我们的超参数，这里我们继承PretrainedConfig类，这是transformers库中的参数类，通过继承这个类，来方便使用transformers库中的一些功能，也方便后续导出Hugging Face模型

```python
from transformers import PretrainedConfig

class ModelConfig(PretrainedConfig):
    
    def __init__(
        self,
        dim: int = 768, # 模型维度
        n_layers: int = 12, # Transformer的层数
        n_heads: int = 16, # 注意力机制的头数
        n_kv_heads: int = 8, # 键值头的数量
        vocab_size: int = 6144, # 词汇表大小
        hidden_dim: int = None, # 隐藏层维度
        multiple_of: int = 64,
        norm_eps: float = 1e-5, # 归一化层的eps
        max_seq_len: int = 512, #最大序列长度
        dropout: float = 0.0, # dropout概率
        flash_attn: bool = True, # 是否使用Flash Attention
        **kwargs,
    ):
        self.dim = dim
        self.n_layers = n_layers
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.multiple_of = multiple_of
        self.norm_eps = norm_eps
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.flash_attn = flash_attn
        super().__init__(**kwargs)

```

### 构建 RMSNorm

RMSNorm 的数学表达式：

$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2 + \epsilon}}\cdot \gamma
$$